{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SIlkuLbhph8"
      },
      "source": [
        "### ```1-LIBRARIES```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrUNORFNhL4M",
        "outputId": "94e08b0a-4665-4a33-ca5c-cabf72a5d677"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x204e34cbe70>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "# from google.colab import drive\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "torch.cuda.empty_cache()\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi-_hynthikO"
      },
      "source": [
        "### ```2-MOUNT DRIVE (if running on colab)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CWnZMqihfRB",
        "outputId": "ed116d6e-e1a0-42ba-9877-9c010aa6c2ff"
      },
      "outputs": [],
      "source": [
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja-6Rs-6XT3Z"
      },
      "source": [
        "#### ```3-addresses (replace the datasets and ground truth directories and the evaluation path with your own addresses). Also put the IHC prediction of each model on BCI dataset both for train and test in each directory. For example the IHC prediction of model1 on the test data should be in model1_dir_test_eval```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QQM_OD-qkD93"
      },
      "outputs": [],
      "source": [
        "model1_dir_train_eval = '\\\\DB1_train_IHC'\n",
        "model1_dir_test_eval = '\\\\DB1_test_IHC'\n",
        "model2_dir_train_eval = '\\\\DB2_train_eval'\n",
        "model2_dir_test_eval = '\\\\DB2_test_eval'\n",
        "model3_dir_train_eval = '\\\\DB3_train_eval'\n",
        "model3_dir_test_eval = '\\\\DB3_test_eval'\n",
        "gt_dir_test  = '\\\\GT_test'\n",
        "gt_dir_train  = '\\\\GT_train'\n",
        "#-----------------------------------------------------------------\n",
        "eval_path = 'result metrics\\\\evaluation.xlsx'\n",
        "weights_dir = 'experiments\\\\weights'\n",
        "experiments_dir = 'experiments\\\\result images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPJ4bA96XYaB"
      },
      "source": [
        "#### ```4-preparing the train and test datasets and the loaders```\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8qnTR3-7iQN2"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, db1_dir, db2_dir, db3_dir, gt_dir, transform=None):\n",
        "\n",
        "        self.db1_dir = db1_dir\n",
        "        self.db2_dir = db2_dir\n",
        "        self.db3_dir = db3_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(db1_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "\n",
        "        # Load images from the three databases\n",
        "        img1 = Image.open(os.path.join(self.db1_dir, img_name)).convert('RGB')\n",
        "        img2 = Image.open(os.path.join(self.db2_dir, img_name)).convert('RGB')\n",
        "        img3 = Image.open(os.path.join(self.db3_dir, img_name)).convert('RGB')\n",
        "\n",
        "        # Load the ground truth image\n",
        "        gt = Image.open(os.path.join(self.gt_dir, img_name)).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "            gt = self.transform(gt)\n",
        "\n",
        "        inputs = torch.stack([img1, img2, img3], dim=-1)  # Stack images along the last dimension\n",
        "        return inputs, gt.unsqueeze(0)\n",
        "    \n",
        "    def get_image_name(self,idx):\n",
        "        return self.image_names[idx]\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = ImageDataset(db1_dir=model1_dir_train_eval, db2_dir=model2_dir_train_eval, db3_dir=model3_dir_train_eval, gt_dir=gt_dir_train, transform=transform)\n",
        "test_dataset = ImageDataset(db1_dir=model1_dir_test_eval, db2_dir=model2_dir_test_eval, db3_dir=model3_dir_test_eval, gt_dir=gt_dir_test, transform=transform)\n",
        "\n",
        "the_batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=the_batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=the_batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMhJISzAY9U4"
      },
      "source": [
        "#### ```5-Model()```\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U29Jdekfll6t"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=3, out_channels=16, kernel_size=3, stride=(1, 2, 2), padding=1),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(16, 32,  kernel_size=3, stride=(1, 2, 2), padding=1),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(32, 64,  kernel_size=3, stride=(1, 2, 2), padding=1),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(64, 32, kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(32, 16,  kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(16, 1,  kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs.permute(0,4,1,2,3)\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x) \n",
        "        return x  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k_bNBLoisSX"
      },
      "source": [
        "model summary\n",
        "```----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "               input      [batch_size, 3, 3, 1024, 1024]\n",
        "               \n",
        "            Conv3d-1       [batch_size, 16, 3, 512, 512]           1,312\n",
        "       BatchNorm3d-2       [batch_size, 16, 3, 512, 512]              32\n",
        "              ReLU-3       [batch_size, 16, 3, 512, 512]               0\n",
        "            Conv3d-4       [batch_size, 32, 3, 256, 256]          13,856\n",
        "       BatchNorm3d-5       [batch_size, 32, 3, 256, 256]              64\n",
        "              ReLU-6       [batch_size, 32, 3, 256, 256]               0\n",
        "            Conv3d-7       [batch_size, 64, 3, 128, 128]          55,360\n",
        "       BatchNorm3d-8       [batch_size, 64, 3, 128, 128]             128\n",
        "              ReLU-9       [batch_size, 64, 3, 128, 128]               0\n",
        "  ConvTranspose3d-10       [batch_size, 32, 3, 256, 256]          55,328\n",
        "      BatchNorm3d-11       [batch_size, 32, 3, 256, 256]              64\n",
        "             ReLU-12       [batch_size, 32, 3, 256, 256]               0\n",
        "  ConvTranspose3d-13       [batch_size, 16, 3, 512, 512]          13,840\n",
        "      BatchNorm3d-14       [batch_size, 16, 3, 512, 512]              32\n",
        "             ReLU-15       [batch_size, 16, 3, 512, 512]               0\n",
        "  ConvTranspose3d-16      [batch_size, 1, 3, 1024, 1024]             433\n",
        "          Sigmoid-17      [batch_size, 1, 3, 1024, 1024]               0\n",
        "================================================================\n",
        "Total params: 140,449\n",
        "Trainable params: 140,449\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 72.00\n",
        "Forward/backward pass size (MB): 1968.00\n",
        "Params size (MB): 0.54\n",
        "Estimated Total Size (MB): 2040.54\n",
        "----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ```6-For training the model from scratch or from the last epoch run the the bottom cell (after running the top cells)```\n",
        "the last weights will be saved after each epoch in the 'weights_dir' directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a27OzJglrUY"
      },
      "outputs": [],
      "source": [
        "#-----------------training method----------------------------\n",
        "def train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs=21, device='cuda', weights_dir=weights_dir):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    total_steps = len(dataloader) * num_epochs\n",
        "    current_step = 0\n",
        "\n",
        "    if not os.path.exists(weights_dir):\n",
        "        os.makedirs(weights_dir)\n",
        "        print(\"directory for weights created:\",weights_dir)\n",
        "    \n",
        "    existing_files = [f for f in os.listdir(weights_dir) if f.endswith('.pth')]\n",
        "    if existing_files:\n",
        "        # Extract epoch numbers and get the maximum\n",
        "        epochs = [int(re.search(r'_epoch_(\\d+)', f).groups()[0]) for f in existing_files]\n",
        "        last_epoch = max(epochs)\n",
        "        print(f\"Resuming training from epoch: {last_epoch + 1}\")\n",
        "        # Load weights from the file\n",
        "        last_weights_file = f\"{weights_dir}/weights_epoch_{last_epoch}.pth\"\n",
        "        model.load_state_dict(torch.load(last_weights_file))\n",
        "        # Set the starting epoch for training\n",
        "        start_epoch = last_epoch + 1\n",
        "        current_step = len(dataloader) * last_epoch\n",
        "    else:\n",
        "        print(\"Initializing weights using Xavier initialization\")\n",
        "        model._initialize_weights()  # Custom function to initialize weights using Xavier initialization\n",
        "        start_epoch = 1  # Start from the first epoch\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            current_step += 1\n",
        "\n",
        "            percentage_done = (current_step / total_steps) * 100\n",
        "\n",
        "            print(f'Epoch {epoch}/{num_epochs}, Step {current_step}/{total_steps} ({percentage_done:.2f}% complete)')\n",
        "\n",
        "            del inputs, targets, outputs\n",
        "            gc.collect()\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f'Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        weights_file = os.path.join(weights_dir, f'weights_epoch_{epoch}.pth')\n",
        "        torch.save(model.state_dict(), weights_file)\n",
        "        print(f'Model weights saved at: {weights_file}')\n",
        "\n",
        "    return model\n",
        "#------------------------------------------------------------------------------------------------------\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EncoderDecoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "model = train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=25, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ```7-For loading the pretrained weights on the model (skip cell 6) run this cell```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_path = 'experiments\\\\weights\\\\weights_epoch_13.pth'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EncoderDecoder().to(device)\n",
        "model.load_state_dict(torch.load(weights_path,map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxmmpf-pc8A9"
      },
      "source": [
        "#### ```8-evaluate() (this will only evaluate on the test data and save the metrics in eval_path (the images will not be saved))```\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Mqc7KwUNdk5G"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    results = []\n",
        "    ssim_change = 0\n",
        "    psnr_change = 0\n",
        "    counter = 0\n",
        "    global_counter = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, gt in dataloader:\n",
        "            inputs, gt = inputs.to(device), gt.to(device)\n",
        "            pred = model(inputs)\n",
        "            pred = pred.cpu().numpy()\n",
        "            gt = gt.cpu().numpy()\n",
        "            inputs = inputs.cpu().numpy()\n",
        "            global_counter += 1\n",
        "            print(f'{(100 * global_counter/(len(dataloader)))} percent evaluated')\n",
        "\n",
        "            for j in range(inputs.shape[0]):\n",
        "\n",
        "              counter += 1\n",
        "              ssim_img1 = (ssim(inputs[j, :, :, :, 0].transpose(1,2,0), gt[j,0,:,:,:].transpose(1,2,0), multichannel=True, channel_axis=2 , data_range=1))\n",
        "              psnr_img1 = (psnr(inputs[j, :, :, :, 0].transpose(1,2,0), gt[j,0,:,:,:].transpose(1,2,0)))\n",
        "\n",
        "              ssim_pred = ssim(pred[j,0,:,:,:].transpose(1, 2, 0), gt[j,0,:,:,:].transpose(1, 2, 0), multichannel=True, channel_axis=2  , data_range=1)\n",
        "              psnr_pred = psnr(pred[j,0,:,:,:], gt[j,0,:,:,:])\n",
        "\n",
        "              results.append((ssim_img1, ssim_pred, psnr_img1, psnr_pred))\n",
        "\n",
        "    df = pd.DataFrame(results, columns=['SSIM Image1', 'SSIM Predicted', 'PSNR Image1', 'PSNR Predicted'])\n",
        "    try:\n",
        "        df.to_excel(eval_path, index=True)\n",
        "        print('metrics saved at: ', eval_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to Excel: {e}\")\n",
        "    print('-------------------------------------------------')\n",
        "    print(df.mean())\n",
        "\n",
        "evaluate_model(model, test_loader, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ```9-evaluate() (this cell will predict the test dataset images (IHC images) and save them in experiments_dir)```\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hePMpxE4f6M9"
      },
      "outputs": [],
      "source": [
        "def predict(model, inputs, device='cuda'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = inputs.to(device)\n",
        "        output = model(inputs)\n",
        "    return output.cpu()\n",
        "\n",
        "def predict_and_save(model, inputs, name ,device='cuda'):\n",
        "    pred = predict(model, inputs, device=device)\n",
        "    pred = pred[0,0,:,:,:].numpy().transpose(1, 2, 0)\n",
        "    inputs = inputs.numpy()\n",
        "    plt.imsave(os.path.join(experiments_dir,name), pred)\n",
        "    print(f'image prediction {name} saved in {experiments_dir}')\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    predict_and_save(model,test_dataset[i][0].unsqueeze(0),test_dataset.get_image_name(i),device = device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bPJ4bA96XYaB",
        "HgOQonAXZe9v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (BCI_1)",
      "language": "python",
      "name": "bci_1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
