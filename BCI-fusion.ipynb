{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SIlkuLbhph8"
      },
      "source": [
        "### ```1-LIBRARIES```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrUNORFNhL4M",
        "outputId": "94e08b0a-4665-4a33-ca5c-cabf72a5d677"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import json\n",
        "import re\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi-_hynthikO"
      },
      "source": [
        "### ```2-MOUNT DRIVE (if running on colab)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CWnZMqihfRB",
        "outputId": "ed116d6e-e1a0-42ba-9877-9c010aa6c2ff"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja-6Rs-6XT3Z"
      },
      "source": [
        "#### ```3-addresses (replace the datasets and ground truth directories and the evaluation path with your own addresses)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQM_OD-qkD93"
      },
      "outputs": [],
      "source": [
        "db1_dir = 'addrDB1'\n",
        "db2_dir = 'addrDB2'\n",
        "db3_dir = 'addrDB3'\n",
        "gt_dir  = 'addrGroundTruth'\n",
        "eval_path = 'evaluation_path/evaluation.xlsx'\n",
        "weights_dir = 'weights_dir'\n",
        "experiments_dir = 'experiments_dir'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPJ4bA96XYaB"
      },
      "source": [
        "#### ```4-datasets and random split```\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qnTR3-7iQN2"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, db1_dir, db2_dir, db3_dir, gt_dir, transform=None):\n",
        "\n",
        "        self.db1_dir = db1_dir\n",
        "        self.db2_dir = db2_dir\n",
        "        self.db3_dir = db3_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(db1_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "\n",
        "        # Load images from the three databases\n",
        "        img1 = Image.open(os.path.join(self.db1_dir, img_name)).convert('RGB')\n",
        "        img2 = Image.open(os.path.join(self.db2_dir, img_name)).convert('RGB')\n",
        "        img3 = Image.open(os.path.join(self.db3_dir, img_name)).convert('RGB')\n",
        "\n",
        "        # Load the ground truth image\n",
        "        gt = Image.open(os.path.join(self.gt_dir, img_name)).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "            img3 = self.transform(img3)\n",
        "            gt = self.transform(gt)\n",
        "\n",
        "        inputs = torch.stack([img1, img2, img3], dim=-1)  # Stack images along the last dimension\n",
        "        return inputs, gt.unsqueeze(0)\n",
        "    \n",
        "    def get_image_name(self,idx):\n",
        "        return self.image_names[idx]\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "dataset = ImageDataset(db1_dir=db1_dir, db2_dir=db2_dir, db3_dir=db3_dir, gt_dir=gt_dir, transform=transform)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "full_data_loader = DataLoader(dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMhJISzAY9U4"
      },
      "source": [
        "#### ```5-Model()```\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U29Jdekfll6t"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=3, out_channels=16, kernel_size=3, stride=(1, 2, 2), padding=1),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(16, 32,  kernel_size=3, stride=(1, 2, 2), padding=1),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(32, 64,  kernel_size=3, stride=(1, 2, 2), padding=1),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose3d(64, 32, kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(32, 16,  kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose3d(16, 1,  kernel_size=3, stride=(1, 2, 2), padding=1, output_padding=(0, 1, 1)),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d) or isinstance(m, nn.ConvTranspose3d):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # torch.Size([BS, 3, 1024, 1024, 3]) ---> [BS, color , H , W , depth] (inputs) --> must be: [BS, depth, color , H , W] = [BS, 3, 3, 1024, 1024]\n",
        "        # torch.Size([BS, 1, 3, 1024, 1024]) ---> [BS, depth , color , H , W]   (GT)\n",
        "        x = inputs.permute(0,4,1,2,3)\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)  # Shape will be (1, 1, new_depth, new_height, new_width)\n",
        "        # x = x.squeeze(1)  # Remove the depth dimension\n",
        "        return x  # Return to shape (H, W, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k_bNBLoisSX"
      },
      "source": [
        "model summary\n",
        "```----------------------------------------------------------------\n",
        "        Layer (type)               Output Shape         Param #\n",
        "================================================================\n",
        "               input      [batch_size, 3, 3, 1024, 1024]\n",
        "               \n",
        "            Conv3d-1       [batch_size, 16, 3, 512, 512]           1,312\n",
        "       BatchNorm3d-2       [batch_size, 16, 3, 512, 512]              32\n",
        "              ReLU-3       [batch_size, 16, 3, 512, 512]               0\n",
        "            Conv3d-4       [batch_size, 32, 3, 256, 256]          13,856\n",
        "       BatchNorm3d-5       [batch_size, 32, 3, 256, 256]              64\n",
        "              ReLU-6       [batch_size, 32, 3, 256, 256]               0\n",
        "            Conv3d-7       [batch_size, 64, 3, 128, 128]          55,360\n",
        "       BatchNorm3d-8       [batch_size, 64, 3, 128, 128]             128\n",
        "              ReLU-9       [batch_size, 64, 3, 128, 128]               0\n",
        "  ConvTranspose3d-10       [batch_size, 32, 3, 256, 256]          55,328\n",
        "      BatchNorm3d-11       [batch_size, 32, 3, 256, 256]              64\n",
        "             ReLU-12       [batch_size, 32, 3, 256, 256]               0\n",
        "  ConvTranspose3d-13       [batch_size, 16, 3, 512, 512]          13,840\n",
        "      BatchNorm3d-14       [batch_size, 16, 3, 512, 512]              32\n",
        "             ReLU-15       [batch_size, 16, 3, 512, 512]               0\n",
        "  ConvTranspose3d-16      [batch_size, 1, 3, 1024, 1024]             433\n",
        "          Sigmoid-17      [batch_size, 1, 3, 1024, 1024]               0\n",
        "================================================================\n",
        "Total params: 140,449\n",
        "Trainable params: 140,449\n",
        "Non-trainable params: 0\n",
        "----------------------------------------------------------------\n",
        "Input size (MB): 72.00\n",
        "Forward/backward pass size (MB): 1968.00\n",
        "Params size (MB): 0.54\n",
        "Estimated Total Size (MB): 2040.54\n",
        "----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ```6-For training the model from scratch run the the bottom cell (after running the top cells)```\n",
        "the last weights will be saved after each epoch in the 'weights_dir' directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a27OzJglrUY"
      },
      "outputs": [],
      "source": [
        "#-----------------training method----------------------------\n",
        "def train_model(model, dataloader, criterion, optimizer, scheduler, num_epochs=21, device='cuda', weights_dir=weights_dir):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    total_steps = len(dataloader) * num_epochs\n",
        "    current_step = 0\n",
        "\n",
        "    # if not os.path.exists(weights_dir):\n",
        "    #     os.makedirs(weights_dir)\n",
        "\n",
        "    # existing_files = [f for f in os.listdir(weights_dir) if f.endswith('.pth')]\n",
        "    # if existing_files:\n",
        "    #     # Extract epoch numbers and get the maximum\n",
        "    #     epochs = [int(re.search(r'_epoch_(\\d+)', f).groups()[0]) for f in existing_files]\n",
        "    #     last_epoch = max(epochs)\n",
        "    #     print(f\"Resuming training from epoch: {last_epoch + 1}\")\n",
        "    #     # Load weights from the file\n",
        "    #     last_weights_file = f\"weights/weights_epoch_{last_epoch}.pth\"\n",
        "    #     model.load_state_dict(torch.load(last_weights_file))\n",
        "    #     # Set the starting epoch for training\n",
        "    #     start_epoch = last_epoch + 1\n",
        "    \n",
        "    print(\"Initializing weights using Xavier initialization\")\n",
        "    model._initialize_weights()  # Custom function to initialize weights\n",
        "    start_epoch = 1  # Start from the first epoch\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            current_step += 1\n",
        "\n",
        "            # Calculate and print the percentage of training done\n",
        "            percentage_done = (current_step / total_steps) * 100\n",
        "\n",
        "            print(f'Epoch {epoch}/{num_epochs}, Step {current_step}/{total_steps} ({percentage_done:.2f}% complete)')\n",
        "\n",
        "            del inputs, targets, outputs\n",
        "            gc.collect()\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f'Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        weights_file = os.path.join(weights_dir, 'weights.pth')\n",
        "        torch.save(model.state_dict(), weights_file)\n",
        "        print(f'Model weights saved at: {weights_file}')\n",
        "\n",
        "    return model\n",
        "#------------------------------------------------------------------------------------------------------\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EncoderDecoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "model = train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=25, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ```7-For loading the pretrained weights on the model (skip cell 6) run this cell```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights_path = '/.../weights_pretrained.pth'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = EncoderDecoder().to(device)\n",
        "model.load_state_dict(torch.load(weights_path,map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxmmpf-pc8A9"
      },
      "source": [
        "#### ```8-evaluate() (this will only evaluate and save the metrics in eval_path (the images will not be saved))```\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqc7KwUNdk5G"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    results = []\n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, gt in dataloader:\n",
        "\n",
        "            inputs, gt = inputs.to(device), gt.to(device)\n",
        "            pred = model(inputs)\n",
        "            pred = pred.cpu().numpy()\n",
        "            gt = gt.cpu().numpy()\n",
        "            inputs = inputs.cpu().numpy()\n",
        "\n",
        "            for j in range(inputs.shape[0]):\n",
        "              \n",
        "              counter += 1\n",
        "              ssim_img1 = (ssim(inputs[j, :, :, :, 0].transpose(1,2,0), gt[j,0,:,:,:].transpose(1,2,0), multichannel=True, channel_axis=2 , data_range=1))\n",
        "              psnr_img1 = (psnr(inputs[j, :, :, :, 0].transpose(1,2,0), gt[j,0,:,:,:].transpose(1,2,0)))\n",
        "              ssim_pred = ssim(pred[j,0,:,:,:].transpose(1, 2, 0), gt[j,0,:,:,:].transpose(1, 2, 0), multichannel=True, channel_axis=2  , data_range=1)\n",
        "              psnr_pred = psnr(pred[j,0,:,:,:], gt[j,0,:,:,:])\n",
        "              results.append((ssim_img1, ssim_pred, psnr_img1, psnr_pred))\n",
        "\n",
        "    df = pd.DataFrame(results, columns=['SSIM Image1', 'SSIM Predicted', 'PSNR Image1', 'PSNR Predicted'])\n",
        "    try:\n",
        "        df.to_excel(eval_path, index=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to Excel: {e}\")\n",
        "\n",
        "    print(df.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ```9-evaluate() (this cell will predict the test_dataset images output and save them in experiments_dir)```\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hePMpxE4f6M9"
      },
      "outputs": [],
      "source": [
        "def predict(model, inputs, device='cuda'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = inputs.to(device)\n",
        "        output = model(inputs)\n",
        "    return output.cpu()\n",
        "\n",
        "\n",
        "def predict_and_save(model, inputs, name ,device='cuda'):\n",
        "    pred = predict(model, inputs, device=device)\n",
        "    pred = pred[0,0,:,:,:].numpy().transpose(1, 2, 0)\n",
        "    inputs = inputs.numpy()\n",
        "    plt.imsave(os.path.join(experiments_dir,name), pred)\n",
        "    print(f'image prediction {name} saved in {experiments_dir}')\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    predict_and_save(model,dataset[i][0].unsqueeze(0),dataset.get_image_name(i),device = device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bPJ4bA96XYaB",
        "HgOQonAXZe9v"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
